from typing import Sequence, TypedDict, Annotated, Literal, List
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, ToolMessage, BaseMessage
from langgraph.graph import START, StateGraph
from langgraph.graph.message import add_messages
from langchain.tools import tool
from langchain_deepseek import ChatDeepSeek
from langgraph.types import Command
from langgraph.prebuilt import ToolNode
from pydantic import BaseModel
from concurrent.futures import ThreadPoolExecutor
from tavily import TavilyClient
import os
from dotenv import load_dotenv
from langchain_core.callbacks import BaseCallbackHandler
from langgraph.checkpoint.memory import MemorySaver

load_dotenv()

memory = MemorySaver()


class DebugCallbackHandler(BaseCallbackHandler):
    def on_chain_start(self, serialized, inputs, **kwargs):
        print(f"Event from {inputs['sender']} from {kwargs['name']} starting")
    
    def on_chain_end(self, outputs, **kwargs):
        print(f"Event is sent to {outputs.goto}")
    
    def on_llm_start(self, prompt, **kwargs):
        print("123")
    
    def on_llm_end(self, response, *, run_id, parent_run_id = None, **kwargs):
        print("234")

config = {
    # "callbacks": [DebugCallbackHandler()],
    "callbacks": [],
    "tags": ["research_agent", "housing_analysis"],
    "metadata": {"query_type": "real_estate"},
    "run_name": "housing_research_workflow"
}


class Clarify_State(TypedDict):
    messages: Annotated[Sequence[BaseMessage], add_messages]
    sender: str

@tool
def clarify_research_topic(question: str)->str:
    '''
    This is a tool function that will take question generated by AI and take user's answer as return
    Args:
        question(str): the question str as clarification question
    Return:
        user_input(str): the feedback from user about the question
    '''
    print(question)
    feedback = input("What is your answer:")
    return feedback

topic_clarify_model = ChatDeepSeek(model="deepseek-chat", api_key=os.getenv("api_key")).bind_tools([clarify_research_topic])

def clarify_agent(state: Clarify_State)->Command[Literal["__end__", "clarify_research_topic_tool"]]:
    '''The clarify agent will take user's question and clarify it via using tool for taking user's feedback is the question is not clear or too big to analyze'''
    system_prompt = """
    You are a topic clarification agent. Analyze if the query is clear enough. YOU DONT have to do the web search another agent will do.
    
    CRITICAL INSTRUCTION: If the query is vague, broad, or lacks specific details, you MUST use the clarify_research_topic tool to ask clarifying questions where users can send further feedback.
    
    MAKE SURE you ALWAYS call the user input tool if you need feedback from user
    """
    system_message = SystemMessage(content=system_prompt)
    all_messages = [system_message] + state["messages"]
    response = topic_clarify_model.invoke(all_messages)
    if hasattr(response, "tool_calls") and response.tool_calls:
        return Command(
            goto="clarify_research_topic_tool",
            update={"messages": state["messages"] + [response], "sender": "clarify_agent"}
        )
    else:
        return Command(goto="__end__", update={
            "messages": state["messages"] + [response],
            "sender":  "clarify_agent"
        })
    


class Topics(BaseModel):
    topics: List[str]

class Summary_State(TypedDict):
    messages: Annotated[Sequence[BaseMessage], add_messages]
    summarized_topics: Sequence[str]
    sender: str

topic_summary_model = ChatDeepSeek(model="deepseek-chat", api_key=os.getenv("api_key")).with_structured_output(Topics)

def topic_summary_agent(state: Summary_State)->Command[Literal["__end__"]]:
    '''
    this topic summary agent is responsible for summarizing the the conversation with user's input into 3 - 4 key topics, the topics must contain what user is truely looking for
    '''
    system_prompt = "This is topic summary agent. For summarizing the conversation into 3 - 4 key topics which will be used for other search agent to process. NOTE: the topics must contain what user is truely looking for"
    
    all_messages = [SystemMessage(content=system_prompt)] + state['messages']
    converted_messages = []
    for message in all_messages:
        if isinstance(message, ToolMessage):
            # ToolMessage â†’ HumanMessage
            converted_messages.append(
                HumanMessage(content=message.content)
            )
        elif isinstance(message, AIMessage):
            # æ£€æŸ¥æ˜¯å¦æœ‰ tool_calls
            if hasattr(message, 'tool_calls') and message.tool_calls:
                # æœ‰ tool_callsï¼šç§»é™¤å®ƒä»¬ï¼Œåªä¿ç•™ content
                if message.content:
                    # åˆ›å»ºæ–°çš„ AIMessageï¼Œä¸åŒ…å« tool_calls
                    converted_messages.append(
                        AIMessage(content=message.content)
                    )
                # å¦‚æœæ²¡æœ‰ contentï¼Œè·³è¿‡è¿™æ¡æ¶ˆæ¯
            else:
                # æ²¡æœ‰ tool_callsï¼Œæ­£å¸¸ä¿ç•™
                converted_messages.append(message)
        else:
            # HumanMessage, SystemMessage - ç›´æ¥ä¿ç•™
            converted_messages.append(message)
    
    response = topic_summary_model.invoke(converted_messages)
    ai_message = AIMessage(content=", ".join(response.topics))
    return Command (
        goto="__end__",
        update={
            "messages": state["messages"] + [ai_message],
            "summarized_topics": response.topics,
            "sender": "topic_summary_agent"
        }
    )

@tool
def search_tool(topics: List[str])->list[dict]:
    '''
    search_tool is for finding the relevant information over the internet via the tavily client
    Args:
        topics(List[str]): the list of search topics that the search tool will need to search the relevant information
    Return:
        a list of dict of the information of each topics
    '''
    tavily_api_key = os.getenv("tavily_api_key")
    client = TavilyClient(api_key=tavily_api_key)
    with ThreadPoolExecutor(max_workers=5) as executor:
        results = list(executor.map(lambda topic: client.search(topic), topics))
    return results

class Search_State(TypedDict):
    messages: Annotated[Sequence[BaseMessage], add_messages]
    search_count: int
    sender: str

search_model = ChatDeepSeek(model="deepseek-chat", api_key=os.getenv("api_key")).bind_tools([search_tool])

def search_agent(state: Search_State)->Command[Literal["search_tool", "__end__"]]:
    '''This search agent will search the relevant information about the search topics via the search tool I provide'''
    system_prompt = "search agent will be using the search tool for fidning the relevant information from the internet." \
    "for search the information you MUST use the search tool I provide"
    
    all_messages = [SystemMessage(content=system_prompt)] + state["messages"]
    response = search_model.invoke(all_messages)
    if hasattr(response, "tool_calls") and response.tool_calls:
        return Command(
            goto="search_tool",
            update={
                "messages": state["messages"] + [response],
                "sender": "search_agent"
            }
        )
    else:
        return Command(
            goto="__end__",
            update={
                "messages": state["messages"] + [response],
                "sender": "search_agent"
            }
        )

class Report_Writer_State(TypedDict):
    messages: Annotated[Sequence[BaseMessage], add_messages]
    sender: str

report_writer_model = ChatDeepSeek(model="deepseek-chat", api_key=os.getenv("api_key"))

def report_writer_agent(state: Report_Writer_State) -> Command[Literal["__end__"]]:
    '''
    The report write agent is for summarizing the search results into a small report
    '''
    system_prompt = "You are professional report writer good at summarizing the previous conversation and search result into a formal report. " \
    "Please make sure you can complete the task and DONT add any fake information there, unless it is a educated guess"
    system_message = SystemMessage(content=system_prompt)
    all_messages = [system_message] + state["messages"]
    response = report_writer_model.invoke(all_messages)
    print(response.text)
    return Command(
        goto="__end__",
        update={
            "messages": state["messages"] + [response],
            "sender": "report_writer_agent"
        }
    )

    

class Supervisor_State(TypedDict):
    messages: Annotated[Sequence[BaseMessage], add_messages]
    sender: str

def supervisor_agent(state: Supervisor_State, config=None)->Command[Literal["clarify_app", "topic_summary_app", "search_app", "report_writer_app"]]:
    '''supervise agent is for routing the message states between different sub-agents for completing the deligated tasks'''
    current_sender = state.get("sender", "unkown")
    last_message = state["messages"][-1] if state["messages"] else None
    is_interruption = (isinstance(last_message, HumanMessage) and  current_sender == "user_interruption")
    if is_interruption:
        print("clarify_app (INTERRUPTION)")
        limited_messages = state['messages'][-5:]
        return Command(goto="clarify_app", update={
            "messages": limited_messages,
            "sender": "supervisor_agent"
        })
    elif state["sender"] == "user":
        return Command(goto="clarify_app", update={
            "messages": state["messages"],
            # "sender": "supervisor_agent"
        })
    elif state["sender"] == "clarify_agent":
        return Command(goto="topic_summary_app", update={
            "messages": state["messages"],
            "sender": "supervisor_agent",
            "summarized_topics": []
        })
    elif state["sender"] == "topic_summary_agent":
        return Command(goto="search_app", update={
            "messages": state["messages"],
            "sender":"supervisor_agent"
        })
    elif state["sender"] == "search_agent":
        return Command(goto="report_writer_app", update={
            "messages": state["messages"],
            "sender": "supervisor_agent"
        })
    elif state["sender"] == "report_writer_agent" :
        return Command(goto="__end__", update={
            "messages": state["messages"],
            "sender": "supervisor_agent"
        })
        

topic_summary_app_graph = StateGraph(Summary_State)
topic_summary_app_graph.add_node("topic_summary_agent", topic_summary_agent)
topic_summary_app_graph.add_edge(START, "topic_summary_agent")
topic_summary_app = topic_summary_app_graph.compile()


report_writer_app_graph = StateGraph(Report_Writer_State)
report_writer_app_graph.add_node("report_writer_agent", report_writer_agent)
report_writer_app_graph.add_edge(START, "report_writer_agent")
report_writer_app = report_writer_app_graph.compile()
    

clarify_app_graph = StateGraph(Clarify_State)
clarify_app_graph.add_node("clarify_agent", clarify_agent)
clarify_app_graph.add_node("clarify_research_topic_tool", ToolNode([clarify_research_topic]))
clarify_app_graph.add_edge(START, "clarify_agent")
clarify_app_graph.add_edge("clarify_research_topic_tool", "clarify_agent")
clarify_app = clarify_app_graph.compile()


search_app_graph = StateGraph(Search_State)
search_app_graph.add_node("search_agent", search_agent)
search_app_graph.add_node("search_tool", ToolNode(tools=[search_tool]))
search_app_graph.add_edge(START, "search_agent")
search_app_graph.add_edge("search_tool", "search_agent")
search_app = search_app_graph.compile()

supervisor_app_graph = StateGraph(Supervisor_State)
supervisor_app_graph.add_node("supervisor_agent", supervisor_agent)
supervisor_app_graph.add_node("clarify_app", clarify_app)
supervisor_app_graph.add_node("topic_summary_app", topic_summary_app)
supervisor_app_graph.add_node("search_app", search_app)
supervisor_app_graph.add_node("report_writer_app", report_writer_app)
supervisor_app_graph.add_edge(START, "supervisor_agent")
supervisor_app_graph.add_edge("clarify_app", "supervisor_agent")
supervisor_app_graph.add_edge("topic_summary_app", "supervisor_agent")
supervisor_app_graph.add_edge("search_app", "supervisor_agent")
supervisor_app_graph.add_edge("report_writer_app", "supervisor_agent")
supervisor_app = supervisor_app_graph.compile(checkpointer=memory)

def demo_checkpoint_resume():
    thread_id = "user_123_session_1"
    print("Starting multi-agent research system with checkpoint")
    print("="*60)
    print("\n First execution:")
    config1 = {
        "configurable": {
            "thread_id": thread_id
        },
        **config
    }
    try:
        result1 = supervisor_app.invoke({"messages": [HumanMessage(content="èƒ½å¸®æˆ‘çœ‹ä¸€ä¸‹47 Darling Street Glebe NSW Sydneyè¿™å¥—æˆ¿å­çš„åŸºæœ¬ä¿¡æ¯ä»¥åŠæœªæ¥çš„èµ°åŠ¿å—ï¼Ÿç”¨äº’è”ç½‘ä¸Šçš„æ•°æ®åšä¸€äº›åˆç†çš„é¢„æµ‹ï¼Œæ¯”å¦‚3å¹´åå–äº†èƒ½ä»·å€¼å¤šå°‘ï¼Ÿæˆ‘æ²¡æœ‰è¿™å¥—æˆ¿å­çš„åŸºæœ¬ä¿¡æ¯ï¼Œè¯·æŸ¥é˜…äº’è”ç½‘ç„¶åå‘Šè¯‰æˆ‘ï¼Œå¹¶ä¸”è€ƒè™‘æ‰€æœ‰å¯èƒ½ä¼šå½±å“æˆ¿ä»·èµ°åŠ¿çš„å› ç´ ï¼Œå¹¶é¢„æµ‹3å¹´å·¦å³çš„èµ°åŠ¿")], "sender": "user"}, config=config1)
        print(f"First execution completed")
        print("\nğŸ”„ USER INTERRUPTION: User wants to clarify")
        
        # ç¬¬äºŒæ¬¡æ‰§è¡Œ - ä½¿ç”¨ç›¸åŒçš„ thread_id æ¢å¤çŠ¶æ€
        print("\nğŸ“ SECOND EXECUTION (Resume from checkpoint):")
        result2 = supervisor_app.invoke({
            "messages": [HumanMessage(content="ç­‰ä¸€ä¸‹ï¼Œæˆ‘å…¶å®æ›´å…³æ³¨æ˜¯glebe è¿™ä¸ªsuburbçš„houseå…¶å®ï¼Œä»¥åŠä»–ä»¬æœªæ¥5å¹´å†…çš„ä»·å€¼èµ°å‘")], 
            "sender": "user_interruption"
        }, config=config1)
        if result2.get('messages'):
            last_message = result2['messages'][-1]
            if isinstance(last_message, AIMessage):
                print("\nğŸ“Š FINAL REPORT:")
                print(last_message.content)
        print(f"Second execution compelted! final sender: {result2.get('sender', 'unknwon')}")
    except Exception as e:
        print(f" Error occurred: {e}")

demo_checkpoint_resume()